\appendix
\titleformat{\section}{\normalfont\large\bfseries}{\centering Приложение \thesection. }{0pt}{\large\centering}
\renewcommand{\thesection}{\Asbuk{section}}
\section{Листинги}

{
	\captionof{lstlisting}{Векторизация голосовых команд и визуализация mfcc}
	\label{vectorize}
	\begin{minted}[frame=lines,fontsize=\footnotesize,breaklines=true,numbers=left]{python}
import os
import soundfile as sf
import librosa
import numpy as np
import matplotlib.pyplot as plt

soundfiles_path = './notebook/'
command_folders = {command : soundfiles_path + command for command in os.listdir(soundfiles_path)}
individual_commands_folders = {}

for command, command_folder in command_folders.items():
	files = os.listdir(command_folder)
	file_paths = []
	for file in files:
	#file_paths = [f'{command_folder}/{file}' for file in files ]
	numbers = set('1234567890')
	symbols_in_file = set(file)
	numbers_in_filename = set.intersection(numbers, symbols_in_file)
	
	if not numbers_in_filename:
		continue
	
	file_path = f'{command_folder}/{file}'
	file_paths.append(file_path)
	
	individual_commands_folders[command] = file_paths

print(command_folders)
print(individual_commands_folders) 

# 1 график
back1_avg = x_avg[33]
back2_avg = x_avg[35]
plt.plot(back1_avg, label='Назад1')
plt.plot(back2_avg, label='Назад2')
plt.legend()
plt.title('Усреднённые mfcc')
plt.show()

# 2 график
around_avg = x_avg[75]
plt.plot(back1_avg, label='Назад')
plt.plot(around_avg, label='Разворот')
plt.legend()
plt.title('Усреднённые mfcc')
plt.show()

vowel_path = './vowel_scan1.wav'
consonant_path = './consonant_scan1.wav'

# 3 график
vowel_mfcc = np.mean(calc_mfcc(vowel_path), axis=1)
consonant_mfcc = np.mean(calc_mfcc(consonant_path), axis=1)

plt.plot(vowel_mfcc, label='Вокализованные')
plt.plot(consonant_mfcc, label='Невокализованные')
plt.legend()
plt.title('Полученные mfcc')
plt.show()
	\end{minted}
}

{
	\captionof{lstlisting}{Классификатор DTW}
	\label{DTW_classifier}
	\begin{minted}[frame=lines,fontsize=\footnotesize,breaklines=true,numbers=left]{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from fastdtw import fastdtw
from tqdm import tqdm

class DTWClassifier:
	def __init__(self):
		pass
	
	@staticmethod
	def dtw_metric(x, y):
		return fastdtw(x, y)[0]   
	
	def fit(self, train_values, targets):
		self.train_values = train_values
		self.targets = targets
	
	def predict(self, value):
		min_metric = np.inf
		min_metric_i = -1
		for i, train_value in enumerate(self.train_values):
			cur_metric = DTWClassifier.dtw_metric(train_value, value)
			if cur_metric < min_metric:
				min_metric = cur_metric
				min_metric_i = i
		
		return self.targets[min_metric_i]
	
	def score(self, values, targets):
		total = len(values)
		correct = 0
		for value, target in tqdm(list(zip(values, targets))):
			predicted_target = self.predict(value)
			if predicted_target == target:
				correct += 1
		
		accuracy = correct / total
		return accuracy
	\end{minted}
}


{
	\captionof{lstlisting}{Нейросетевой классификатор}
	\label{NN_classifier}
	\begin{minted}[frame=lines,fontsize=\footnotesize,breaklines=true,numbers=left]{python}
from tensorflow.keras.regularizers import l2
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder  

class NN:
	def initialize_nn_model(self, x_train):
		norm = tf.keras.layers.Normalization(axis=-1)
		norm.adapt(np.array(x_train))
		
		self.model = tf.keras.models.Sequential([
			tf.keras.Input(shape=(np.array(x_train).shape[1],)),
			norm,
			tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=l2(1e-5)),
			tf.keras.layers.Dense(16, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(1e-5)),
			tf.keras.layers.Dense(12, activation='softmax')
		])
		
		self.model.compile(optimizer='adam',
			loss='sparse_categorical_crossentropy',
			metrics=['accuracy']
		)
	
	
	def fit(self, x_train, y_train):
		x_train = np.array(x_train)
		y_train = np.array(y_train)
		self.initialize_nn_model(x_train)
		
		early_stop = tf.keras.callbacks.EarlyStopping(
			monitor='val_loss',
			patience=3,
			restore_best_weights=False
		)
		
		self.le = LabelEncoder()
		y_train_enc = self.le.fit_transform(y_train)
		self.model.fit(np.array(x_train), 
			np.array(y_train_enc), 
			epochs=1000,
			validation_split=0.2,
			callbacks=[early_stop]
		)
	
	def evaluate(self, x, y):
		y = np.array(y)
		y_enc = self.le.transform(y)
		return self.model.evaluate(np.array(x), np.array(y_enc))
	
	def predict(self, x):
		x = np.array(x)
		return self.model.predict(x)
	
	def score(self, x, y):
		return self.evaluate(x, y)[1]
	\end{minted}
}

{
	\captionof{lstlisting}{Pipeline для общего интерфейса работы с обоими классификаторами}
	\label{Pipeline_code}
	\begin{minted}[frame=lines,fontsize=\footnotesize,breaklines=true,numbers=left]{python}
from time import perf_counter

class Pipeline:
	def __init__(self, classifier=DTWClassifier, intersection=0.5, n_mfcc=12):
		self.classifier = classifier()
		self.intersection = intersection
		self.n_mfcc = n_mfcc
		
	def fit(self, x, targets):
		self.classifier.fit(x, targets)
	
	def predict(self, x):
		return self.classifier.predict(x)
	
	def score(self, x, targets):
		return self.classifier.score(x, targets)
	
	def generate_fit_score(self):
		_, targets, x_avg = calc_mfccs_from_files(individual_commands_folders, intersection=self.intersection, n_mfcc=self.n_mfcc)
		x_train, x_test, y_train, y_test = get_train_test(x_avg, targets)
		
		self.fit(x_train, y_train)
		
		score_start = perf_counter()
		accuracy = self.score(x_test, y_test)
		score_end = perf_counter()
		
		time_spent_ms = (score_end - score_start) * 1000
		
		return accuracy, time_spent_ms

	\end{minted}
}

{
	\captionof{lstlisting}{Тесты качества классификаторов}
	\label{Classifiers_tests}
	\begin{minted}[frame=lines,fontsize=\footnotesize,breaklines=true,numbers=left]{python}
import pandas as pd
pd.set_option('display.expand_frame_repr', False)  # отключить перенос строк

def get_train_test(x, targets, test_size=0.2, random_state=42):
	max_len = 0
	for x_elem in x:
		cur_len = len(x_elem)
		if cur_len > max_len:
			max_len = cur_len
	
	x_unified = []
	for x_elem in x:
		need_zeros = max_len - len(x_elem)
		if need_zeros:
			x_unified.append(np.pad(x_elem, (0, need_zeros), mode='constant'))
		else:
			x_unified.append(x_elem)
	return train_test_split(x_unified, targets, test_size=test_size, random_state=random_state)

x_train, x_test, y_train, y_test = get_train_test(x_avg, targets)

# Списки значений параметров
intersections = [0.25, 0.5, 0.75]   # Степень перекрытия
n_mfcc_list = [6, 12, 18]           # Кол-во признаков MFCC

# Пустой список для хранения результатов
results = []

# Перебор комбинаций
idx = 1
for intersection in intersections:
	for n_mfcc in n_mfcc_list:
		# DTW
		pipeline_dtw = Pipeline(DTWClassifier, intersection=intersection, n_mfcc=n_mfcc)
		acc_dtw, time_dtw = pipeline_dtw.generate_fit_score()
		
		# NN
		pipeline_nn = Pipeline(NN, intersection=intersection, n_mfcc=n_mfcc)
		acc_nn, time_nn = pipeline_nn.generate_fit_score()
		
		# Добавляем строку в список
		results.append({
			"№": idx,
			"Степень перекрытия": intersection,
			"Кол-во инф. признаков": n_mfcc,
			"DTW точность": acc_dtw,
			"NN точность": acc_nn,
			"DTW время, мс": time_dtw,
			"NN время, мс": time_nn
		})
		idx += 1

# Создаем DataFrame
df = pd.DataFrame(results)

# Смотрим результат
print(df)
	\end{minted}
}

{
	\captionof{lstlisting}{Графики метрик классификаторов}
	\label{Classifiers_graphs}
	\begin{minted}[frame=lines,fontsize=\footnotesize,breaklines=true,numbers=left]{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Предположим, df — твой датафрейм
sns.set(style="whitegrid")

# --- График точности DTW ---
plt.figure(figsize=(8,5))
	sns.lineplot(data=df,
	x="Кол-во инф. признаков",
	y="DTW точность",
	hue="Степень перекрытия",
	marker="o",
	palette="viridis")  # разные цвета для разных степеней перекрытия
plt.title("DTW: Точность vs Кол-во признаков")
plt.xlabel("Количество признаков")
plt.ylabel("Точность")
plt.legend(title="Степень перекрытия")
plt.show()

# --- График точности NN ---
plt.figure(figsize=(8,5))
sns.lineplot(data=df,
	x="Кол-во инф. признаков",
	y="NN точность",
	hue="Степень перекрытия",
	marker="s",
	palette="plasma")
plt.title("NN: Точность vs Кол-во признаков")
plt.xlabel("Количество признаков")
plt.ylabel("Точность")
plt.legend(title="Степень перекрытия")
plt.show()

# --- График времени DTW ---
plt.figure(figsize=(8,5))
sns.lineplot(data=df,
	x="Кол-во инф. признаков",
	y="DTW время, мс",
	hue="Степень перекрытия",
	marker="o",
	palette="viridis")
plt.title("DTW: Время выполнения vs Кол-во признаков")
plt.xlabel("Количество признаков")
plt.ylabel("Время, мс")
plt.legend(title="Степень перекрытия")
plt.show()

# --- График времени NN ---
plt.figure(figsize=(8,5))
sns.lineplot(data=df,
	x="Кол-во инф. признаков",
	y="NN время, мс",
	hue="Степень перекрытия",
	marker="s",
	palette="plasma")
plt.title("NN: Время выполнения vs Кол-во признаков")
plt.xlabel("Количество признаков")
plt.ylabel("Время, мс")
plt.legend(title="Степень перекрытия")
plt.show()
	\end{minted}
}